{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Майнор ВШЭ\n", "## Прикладные задачи анализа данных 2020\n", "## Семинар 11: Регулярные выражения. Токенизация. Лемматизация. Стемминг. Модель биграмм.\n", "<br>"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["# скачиваем данные и доставляем необходимые библиотеки\n", "!wget https://raw.githubusercontent.com/hse-ds/iad-applied-ds/master/2020/seminars/seminar11/alice.txt -N\n", "!wget https://raw.githubusercontent.com/hse-ds/iad-applied-ds/master/2020/seminars/seminar11/dinos.txt -N\n", "!pip install nltk\n", "!pip install pymystem3==0.1.10\n", "!pip install pymorphy2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Регулярные выражения\n", "\n", "Регулярные выражения (regular expressions, RegExp) —\n", "это формальный язык для операций с подстроками.\n", "\n", "Чаще всего регулярные выражения используются для:\n", "* поиска в строке;\n", "* разбиения строки на подстроки;\n", "* замены части строки;\n", "* валидации (проверки).\n", "\n", "## Синтаксис:\n", "<img src='https://raw.githubusercontent.com/hse-ds/iad-applied-ds/master/2020/seminars/seminar11/r.png'>\n", "\n", "\n", "\n", "## Онлайн упражнения\n", "https://regexone.com/\n", "<br>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Регулярные выражения в Python\n", "\n", "Основные методы:\n", "```\n", "• re.match() — поиск совпадения в начале строки\n", "• re.search() — поиск первого совпадения\n", "• re.findall() — поиск всех совпадений (возвр. список)\n", "• re.split() — разбиение строки\n", "• re.sub() — замена подстроки\n", "```"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": ["import re"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## match\n", "ищет по заданному шаблону в начале строки"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": ["result = re.match('ab+c.', 'abcdefghijkabcabcd') # ищем по шаблону 'ab+c.' \n", "print (result) # совпадение найдено:"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": ["print(result.group(0)) # выволмс найденное совпадение"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["result = re.match('abc.', 'abdefghijkabcabc')\n", "print(result) # совпадение не найдено"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 1:\n", "Проверьте, начинаются ли строки c заглавной буквы и если да, то вывести эту заглавную букву. Придумайте свои примеры строк для проверки."]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## search\n", "ищет по всей строке, возвращает только первое найденное совпадение"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 2\n", "Проверьте, есть ли в строке вопросительный знак. Придумайте свои примеры для проверки."]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## findall\n", "возвращает список всех найденных совпадений"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n", "print(result)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Вопросы: \n", "1) почему нет последнего abc?\n", "2) почему нет abcx?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 3\n", "Вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["str_ = \"Я, ты, мы, вы, они. Случайность? Не думаю!\"\n", "####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## split\n", "разделяет строку по заданному шаблону\n"]}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": ["result = re.split(',', 'itsy, bitsy, teenie, weenie') \n", "print(result)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["можно указать максимальное количество разбиений"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": ["result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit = 2) \n", "print(result)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 4\n", "Разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["text = \"\"\"\n", "Нашел он полон двор услуги;\n", "К покойнику со всех сторон\n", "Съезжались недруги и други,\n", "Охотники до похорон.\n", "Покойника похоронили.\n", "Попы и гости ели, пили\n", "И после важно разошлись,\n", "Как будто делом занялись.\n", "Вот наш Онегин — сельский житель,\n", "Заводов, вод, лесов, земель\n", "Хозяин полный, а досель\n", "Порядка враг и расточитель,\n", "И очень рад, что прежний путь\n", "Переменил на что-нибудь.\"\"\"\n", "####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## sub\n", "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n", "\n", "параметры: (pattern, repl, string)"]}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [], "source": ["result = re.sub('a', 'b', 'abcabc')\n", "print (result)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 5:\n", "Замените все цифры на звездочки."]}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [], "source": ["str_ = \"7253dvhfks0340921934uhgj34l4321\"\n", "####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## compile\n", "компилирует регулярное выражение в отдельный объект"]}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [], "source": ["# Пример: построение списка всех слов строки:\n", "prog = re.compile('[А-Яа-яё\\-]+')\n", "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 6\n", "Для выбранной строки постройте список слов, которые длиннее трех символов."]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 7\n", "Вернуть первое слово строки"]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 8\n", "Вернуть список доменов (@gmail.com, @rest.biz и т.д.) из списка адресов электронной почты:\n", "\n", "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz"]}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Токенизация\n", "\n", "\n", "Самый наивный способ токенизировать текст -- разделить с помощью split. Но split упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "### Задание 9\n", "\n", "Откройте и запишите в одну текстовую строку файл alice.txt.\n", "\n", "Ответьте на вопросы:\n", "\n", "    Сколько всего символов в файле?\n", "    Какой текст написан в последней строке файла? (Подсказка: в какой переменной содержится эта строка?)\n", "\n"]}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "### Задание 10\n", "\n", "Создайте и скомпилируйте регулярное выражения для разбиения полученной строки на токены. Не забудьте про дефис и букву ё.\n", "\n", "Найдите все токены с помощью этого регулярного выражения, ответьте на вопросы:\n", "\n", "    Сколько токенов получилось?\n", "    Какой токен первый?\n", "    Какой токен последний?\n", "\n"]}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Токенизация в nltk"]}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [], "source": ["from nltk.tokenize import word_tokenize\n", "import nltk\n", "nltk.download('punkt')"]}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": ["example = 'Но не каждый хочет что-то исправлять:('\n", "word_tokenize(example)"]}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [], "source": ["from nltk import tokenize\n", "dir(tokenize)[:16]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Они умеют выдавать индексы начала и конца каждого токена:"]}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [], "source": ["wh_tok = tokenize.WhitespaceTokenizer()\n", "list(wh_tok.span_tokenize(example))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(если вам было интересно, зачем вообще включать в модуль токенизатор, который работает как .split() :))\n", "\n", "Некторые токенизаторы ведут себя специфично:"]}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [], "source": ["tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Для некоторых задач это может быть полезно.\n", "\n", "А некоторые -- вообще не для текста на естественном языке (не очень понятно, зачем это в nltk :)):"]}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [], "source": ["tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Лемматизация\n", "\n", "Лемматизация – это сведение разных форм одного слова к начальной форме – лемме. Почему это хорошо?\n", "\n", "    Во-первых, мы хотим рассматривать как отдельную фичу каждое слово, а не каждую его отдельную форму.\n", "    Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n", "\n", "Для русского есть два хороших лемматизатора: mystem и pymorphy:\n", "\n", "\n", "### Mystem\n", "\n", "https://yandex.ru/dev/mystem/doc/index-docpage/\n", "\n", "Как с ним работать:\n", "* можно скачать mystem и запускать из терминала с разными параметрами\n", "* pymystem3 - обертка для питона, работает медленнее, но это удобно"]}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [], "source": ["from pymystem3 import Mystem\n", "mystem_analyzer = Mystem()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n", "\n", "    mystem_bin - путь к mystem, если их несколько\n", "    grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n", "    disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n", "    entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n", "\n", "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n", "\n", "Можно просто лемматизировать текст:"]}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [], "source": ["print(example)\n", "print(mystem_analyzer.lemmatize(example))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["А можно получить грамматическую информацию:"]}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [], "source": ["mystem_analyzer.analyze(example)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Стоп-слова и пунктуация\n", "\n", "Стоп-слова -- это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": []}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [], "source": ["from nltk.corpus import stopwords\n", "import nltk\n", "\n", "nltk.download('stopwords')\n", "stopwords.words('russian')"]}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [], "source": ["from string import punctuation\n", "punctuation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Реализуем функцию для препроцессинга текста, будем удалять стоп слова и пунктуацию, а в качестве токенизатора и лемматизатора воспользуемся майстемом."]}, {"cell_type": "code", "execution_count": 80, "metadata": {}, "outputs": [], "source": ["import re\n", "def my_preproc_mystem(text):\n", "    text = re.sub('[{}]'.format(punctuation), '', text)\n", "    text = mystem_analyzer.lemmatize(text)\n", "    return [word for word in text if word not in stopwords.words('russian') + [' ', '\\n']]\n", "\n", "print(my_preproc_mystem(example))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Pymorphy\n", "\n", "Это модуль на питоне, довольно быстрый и с кучей функций.\n", "\n", "https://pymorphy2.readthedocs.io/en/latest/index.html"]}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [], "source": ["from pymorphy2 import MorphAnalyzer\n", "pymorphy2_analyzer = MorphAnalyzer()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает"]}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [], "source": ["ana = pymorphy2_analyzer.parse(example)\n", "ana"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ana[0].normal_form"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 11. Реализуйте функцию ```my_preproc_pymorphy``` по аналогии с ```my_preproc_mystem```"]}, {"cell_type": "code", "execution_count": 81, "metadata": {}, "outputs": [], "source": ["def my_preproc_pymorphy(text):\n", "    ####### Здесь ваш код ##########\n", "    raise NotImplementedError\n", "    ################################\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Сравним результаты:"]}, {"cell_type": "code", "execution_count": 86, "metadata": {}, "outputs": [], "source": ["print(f'Mystem: {my_preproc_mystem(example)}')\n", "print(f'PyMorphy: {my_preproc_pymorphy(example)}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Частотность слов можно посмотреть, с помощью ```nltk.FreqDist```. Выведем топ 20 самых популярных токенов до и  после обработки:"]}, {"cell_type": "code", "execution_count": 94, "metadata": {}, "outputs": [], "source": ["from nltk.tokenize import word_tokenize\n", "tokenized = word_tokenize(s)\n", "alice_d_tokenized = nltk.FreqDist(tokenized)\n", "alice_d_tokenized.most_common(20)"]}, {"cell_type": "code", "execution_count": 95, "metadata": {}, "outputs": [], "source": ["alice_preproc = my_preproc_pymorphy(s)\n", "\n", "alice_d = nltk.FreqDist(alice_preproc)\n", "alice_d.most_common(20)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 12. Сколько раз встречается слово встречалось слово 'крокет' до и после обработки? Найдите изначальные слова, используя ```re.findall()```"]}, {"cell_type": "code", "execution_count": 110, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Mystem vs. Pymorphy\n", "\n", "1) Mystem работает невероятно медленно под windows на больших текстах.\n", "\n", "2) Снятие омонимии. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту:\n"]}, {"cell_type": "code", "execution_count": 62, "metadata": {}, "outputs": [], "source": ["homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n", "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n", "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n", "\n", "print(mystem_analyzer.analyze(homonym1)[-5])\n", "print(mystem_analyzer.analyze(homonym2)[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Стэммминг\n", "\n", "Для русского языка можно воспользоваться SnowballStemmer из пакета nltk.stem"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from nltk.stem.snowball import SnowballStemmer\n", "\n", "stemmer = SnowballStemmer(\"russian\")\n", "\n", "plurals = ['мухи', \"умирает\", \"мулы\", \"отказано\", \"умер\", \n", "           \"согласился\", \"владел\", \"унижен\", \"измерен\", \"встреча\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["singles = [stemmer.stem(plural) for plural in plurals]\n", "singles"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Языковые модели. Модель биграм\n", "\n", "Какое слово в последовательности вероятнее: \n", "\n", "Поезд прибыл на\n", "* вокзал\n", "* север\n", "\n", "Какая последовательность вероятнее:\n", "* Вокзал прибыл поезд на\n", "* Поезд прибыл на вокзал\n", "\n", "### Приложения:\n", "* Задачи, в которых нужно обработать сложный и зашумленный вход: распознавание речи, распознавание сканированных и рукописных текстов;\n", "* Исправление опечаток\n", "* Машинный перевод\n", "* Подсказка при наборе \n", "\n", "### Виды моделей:\n", "* Счетные модели\n", "    * цепи Маркова\n", "* Нейронные модели, обычно реккурентные нейронные сети с LSTM/GRU\n", "* seq2seq архитектуры\n", "\n", "### Модель n-gram:\n", "\n", "Пусть $w_{1:n}=w_1,\\ldots,w_m$ – последовательность слов.\n", "\n", "Цепное правило:\n", "$ P(w_{1:m}) = P(w_1) P(w_2 | w_1) P(w_3 | w_{1:2}) \\ldots P(w_m | w_{1:m-1}) = \\prod_{k=1}^{m} P(w_k | w_{1:k-1}) $\n", "\n", "Но оценить $P(w_k | w_{1:k-1})$ не легче! \n", "\n", "Переходим к $n$-грамам: $P(w_{i+1} | w_{1:i}) \\approx P(w_{i+1} | w_{i-n:i})  $ , то есть, учитываем $n-1$ предыдущее слово. \n", "\n", "Модель\n", "* униграм: $P(w_k)$\n", "* биграм: $P(w_k | w_{k-1})$\n", "* триграм: $P(w_k | w_{k-1} w_{k-2})$\n", "\n", "\n", "Т.е. используем Марковские допущения о длине запоминаемой цепочки.\n", "\n", "* Вероятность следующего слова в последовательности: $ P(w_{i+1} | w_{1:i}) \\approx P(w_{i-n:i}) $\n", "* Вероятность всей последовательности слов: $P(w_{1:n}) = \\prod_{k=1}^l P(w_k | w_{k-n+1: k-1}) $\n", "\n", "### Качество модели  $n$-грам\n", "\n", "Перплексия: насколько хорошо модель предсказывает выборку. Чем ниже значение перплексии, тем лучше.\n", "\n", "$PP(\\texttt{LM}) = 2 ^ {-\\frac{1}{m} \\log_2 \\texttt{LM} (w_i | w_{1:i-1})}$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Рассмотрим модель биграм, используя библиотеку NLTK"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open(\"dinos.txt\") as f:\n", "    names = [name.strip().lower() for name in f.readlines()]\n", "    print(names[:10])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Получить информацию о количестве символов можно, используя ```nltk.FreqDist```:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chars = [char  for name in names for char in name]\n", "freq = nltk.FreqDist(chars)\n", "\n", "freq"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Наиболее вероятный следующий токен можно получить с помощью  ```nltk.ConditionalFreqDist``` и ```nltk.ConditionalProbDist```:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cfreq = nltk.ConditionalFreqDist(nltk.bigrams(chars))\n", "cfreq['a']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cprob = nltk.ConditionalProbDist(cfreq, nltk.MLEProbDist)\n", "print('p(a,u) = %1.4f' % cprob['a'].prob('u'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Можно порождать случайные символы с учётом предыдущих:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cprob['a'].generate()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 13. Напишите функцию для оценки вероятности имени динозавра и найдите наиболее вероятное имя из загруженного списка."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Задание 14. Напишите функцию для генерации  имени динозавра фиксированной длины."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["\n", "\n", ""]}], "metadata": {"anaconda-cloud": {}, "celltoolbar": "Raw Cell Format", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.5"}}, "nbformat": 4, "nbformat_minor": 2}