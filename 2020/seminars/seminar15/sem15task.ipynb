{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "TDqfgGd0bL3C"}, "source": ["## Майнор ВШЭ\n", "## Прикладные задачи анализа данных 2020\n", "\n", "### Семинар 15: Обучение с подкреплением: Марковский процесс принятия решений, фреймворк Open AI gym, Q-обучение, аппроксимация Q-функции. \n", "\n", "Обучение с подкреплением (RL - Reinforcement Learning) является направлением машинного обучения и изучает взаимодействие агента, которому необходимо максимизировать долговременный выигрыш в некоторой среде. Агенту не сообщается сведений о правильности действий, как в большинстве задач машинного обучения, вместо этого агент должен определить выгодные действия самостоятельно испробовав их. Испытание действий и отсроченная награда являются основными отличительными признаками RL.\n", "-\n", "<img src=\"https://github.com/hse-ds/iad-applied-ds/raw/master/2020/seminars/seminar15/rlIntro.png\" caption=\"Взаимодействия агента со средой\" style=\"width: 300px;\" />\n", "\n", "Основные составляющие модели RL:\n", "* $s_t$ -- состояние среды в момент времени $t$,\n", "* $a_t$ -- действие, совершаемое агентом в момент времени $t$,\n", "* $r_t$ -- вознаграждение, получаемое агентом при совершении действия $a_t$,\n", "* $\\pi$ -- стратегия, отвечает за выбор действия в конкретном состоянии.\n", "\n", "## 1. Марковский процесс принятия решений\n", "В простейших моделях RL среда представляется в виде марковского процесса принятия решений (MDP), где функция перехода определяется как $P(s' |s,a)$, что означает вероятность оказаться в состоянии $s'$ при совершении действия $a$ в состоянии $s$. Вознаграждение теперь определяется как $r(s,a,s')$.\n", "\n", "<img src=\"https://github.com/hse-ds/iad-applied-ds/raw/master/2020/seminars/seminar15/mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\"/>"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "v5d6HqLaqPLx"}, "source": ["## 2. Open AI gym"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "OFfz5mCmbL3F"}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "import random\n", "from IPython.display import clear_output\n", "%matplotlib inline"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "TCVkq2fwb9R_"}, "outputs": [], "source": ["#устанавливаем библиотеки для визуализации в colab\n", "!apt-get -qq -y install  libnvtoolsext1 > /dev/null\n", "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n", "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n", "!pip -q install gym\n", "!pip -q install pyglet\n", "!pip -q install pyopengl\n", "!pip -q install pyvirtualdisplay==0.2.5\n", "\n", "import matplotlib.pyplot as plt\n", "import matplotlib.animation\n", "import numpy as np\n", "from IPython.display import HTML"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "2zSbMzMUc7gl"}, "outputs": [], "source": ["# запускаем virtual display\n", "from pyvirtualdisplay import Display\n", "display = Display(visible=0, size=(1024, 768))\n", "display.start()\n", "import os\n", "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n", "\n", "\n", "def show_animation(frames):\n", "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n", "    patch = plt.imshow(frames[0])\n", "    plt.axis('off')\n", "    animate = lambda i: patch.set_data(frames[i])\n", "    ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n", "    return ani"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "SNToaC8SbL3K"}, "source": ["На семинаре мы будем пользоваться стандартными средами, реализованными в библиотеке OpenAI Gym (https://gym.openai.com)."]}, {"cell_type": "code", "execution_count": 51, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 269}, "colab_type": "code", "id": "faOB1Hf4bL3K", "outputId": "64398a29-fa38-4e6d-f566-ffb9bccc383a"}, "outputs": [], "source": ["import gym\n", "\n", "# создаем окружение\n", "env = gym.make(\"MountainCar-v0\")\n", "env.reset()\n", "# рисуем картинку\n", "plt.imshow(env.render('rgb_array'))\n", "env.close()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "b8tZM9TZbL3P"}, "source": ["Основные методы класса Env:\n", "\n", "* reset() - инициализация окружения, возвращает первое наблюдение (состояние)\n", "* render() - визуализация текущего состояния среды\n", "* step($a$) - выполнить в среде действие a и получить кортеж: $<s_{t+1}, r_t, done, info>$, где done флаг заверешния, а info - дополнительная информация.\n", "\n", "Прежде чем начать взаимодействие с окружением, нужно использовать метод reset():"]}, {"cell_type": "code", "execution_count": 52, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 67}, "colab_type": "code", "id": "ivlciUwGbL3P", "outputId": "e454fa7a-fe10-4e63-8bd8-ecd178c8fdef"}, "outputs": [], "source": ["state0 = env.reset()\n", "print(\"изначальное состояние среды:\", state0)\n", "# выполняем действие 2 \n", "new_state, reward, done, _ = env.step(2)\n", "print(\"новое состояние:\", new_state)\n", "print(\"вознаграждение\", reward)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "d3GqtZB6bL3T"}, "source": ["### Среда MountainCar-v0\n", "* Состояния: Type: Box(2)\n", "\n", "\n", "\n", "Num | Observation  | Min  | Max  \n", "----|--------------|------|----   \n", "0   | position     | -1.2 | 0.6\n", "1   | velocity     | -0.07| 0.07\n", "\n", "\n", "* Действия: Type: Discrete(3)\n", "\n", "\n", "\n", "Num | Action|\n", "----|-------------|\n", "0   | push left   |\n", "1   | no push     |\n", "2   | push right  |\n", "\n", "* Вознаграждения: -1 за каждый шаг, пока не достигнута цель \n", "\n", "* Начальное состояние: Случайная позиция от -0.6 до -0.4 с нулевой скоростью."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "4R7ScpfFbL3T"}, "source": ["### Задание 1\n", "В среде MountainCar-v0 мы хотим, чтобы тележка достигла флага. Давайте решим эту задачу, не используя обучение с подкреплением. Модифицируйте код функции act() ниже для выполнения этого задания. Функция получает на вход состояние среды и должна вернуть действие. "]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "C8aUhYCUbL3U"}, "outputs": [], "source": ["def act(s):\n", "    actions = {'left': 0, 'stop': 1, 'right': 2}\n", "    \n", "    # пример: можем попробовать ехать только влево\n", "    # action = actions['left'] \n", "    ####### Здесь ваш код ##########\n", "    raise NotImplementedError\n", "    ################################\n", "    return action"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "WzHsoEhRbL3X"}, "source": ["Проверяем решение:"]}, {"cell_type": "code", "execution_count": 54, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 836}, "colab_type": "code", "id": "HnKJDu5ebL3Y", "outputId": "ca1eedcd-3634-428a-bd1c-69577a3c7da5"}, "outputs": [], "source": ["# создаем окружение, с ограничением на число шагов, используя wrapper (обертку) TimeLimit\n", "env = gym.wrappers.TimeLimit(gym.make(\"MountainCar-v0\"), max_episode_steps=250)\n", "# проводим инициализацию и запоминаем начальное состояние\n", "s = env.reset()\n", "done = False\n", "\n", "frames = []\n", "\n", "frames.append(env.render(mode = 'rgb_array'))\n", "\n", "while not done:\n", "    # выполняем действие, получаем s, r, done\n", "    s, r, done, _ = env.step(act(s))\n", "    frames.append(env.render(mode = 'rgb_array'))\n", "    # визуализируем окружение\n", "    env.render()\n", "\n", "env.close()\n", "if s[0] > 0.47:\n", "    print(\"Задание выполнено!\")\n", "else:\n", "    print(\"\"\"Исправьте функцию выбора действия!\"\"\")\n", "\n", "HTML(show_animation(frames).to_jshtml())"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "ajUaHPOJH-3Z"}, "source": ["## 3. Q-обучение\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Одним из наиболее популярных алгоритм обучения на основе временных различий является Q-обучение.Уравнение Беллмана для значения Q-функции записывается как:\n", "\n", "$$Q(s,a)=r(s)+\\gamma\\sum_s'T(s,a,s')\\max_{a'}Q(a',s')$$\n", "\n", "Уравнение для итерационного обновления значений Q-функции выглядит следующим образом:$$Q(s,a)\\leftarrow Q(s,a)+\\alpha(r(s)+\\gamma\\max_{a'}Q(a',s') - Q(s,a)).$$\n", "\n", "Алгоритм Q-обучения:\n", "<img src=\"https://github.com/hse-ds/iad-applied-ds/raw/master/2020/seminars/seminar15/q.png\">\n", "где: s - текущее состояние, a - текущее действие, s', a' - состояние и действие на следующем шаге"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "WJ79cSTLLPb-"}, "source": ["Для обучения будем использовать среду Taxi-v3. Подробнее про данное окружение можно посмотреть в документации: https://gym.openai.com/envs/Taxi-v3/."]}, {"cell_type": "code", "execution_count": 71, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 151}, "colab_type": "code", "id": "ud0_kfOGLUwo", "outputId": "64e98d2b-78d5-4381-b59d-260beaaa22db"}, "outputs": [], "source": ["env = gym.make(\"Taxi-v3\")\n", "\n", "env.render()"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "Phe3a5aTQBsd"}, "outputs": [], "source": ["def show_progress(rewards_batch, log, reward_range=None):\n", "    \"\"\"\n", "    Удобная функция, которая отображает прогресс обучения.\n", "    \"\"\"\n", "\n", "    if reward_range is None:\n", "        reward_range = [-990, +10]\n", "    mean_reward = np.mean(rewards_batch)\n", "    log.append([mean_reward])\n", "\n", "    clear_output(True)\n", "    plt.figure(figsize=[8, 4])\n", "    plt.subplot(1, 2, 1)\n", "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n", "    plt.legend(loc=4)\n", "    plt.grid()\n", "\n", "    plt.grid()\n", "\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "_bddYTsDQmVL"}, "outputs": [], "source": ["# определяем память, в которой будет храниться Q(s,a)\n", "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n", "show_progress\n", "import random\n", "from IPython.display import clear_output\n", "\n", "# гиперпараметры алгоритма\n", "alpha = 0.1\n", "gamma = 0.6\n", "epsilon = 0.1\n", "\n", "episodes_number = 10001\n", "\n", "log = []\n", "rewards_batch = []"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "pG9YEmftQtu0"}, "source": ["### Задание 2\n", "\n", "Напишите код для формулы Q-обновления, используя известные: alpha, reward, gamma, next_max, old_value (q_table[state, action])"]}, {"cell_type": "code", "execution_count": 73, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 282}, "colab_type": "code", "id": "DUthRLiaIuuV", "outputId": "aa3b5a76-dad4-40bc-803b-45ebcb308256"}, "outputs": [], "source": ["for i in range(1, episodes_number):\n", "    state = env.reset()\n", "\n", "    episode, reward, episode_reward = 0, 0, 0\n", "    done = False\n", "    \n", "    while not done:\n", "        # выбираем действие, используя eps-greedy исследование среды\n", "        if random.uniform(0, 1) < epsilon:\n", "            action = env.action_space.sample() # исследуем среду\n", "        else:\n", "            action = np.argmax(q_table[state]) # используем Q-функциthresholdю\n", "\n", "        # выполняем действие в среде \n", "        next_state, reward, done, info = env.step(action) \n", "        \n", "        # получаем old_value (Q(s,a)) и next_max (max(Q(s', a')))\n", "        old_value = q_table[state, action]\n", "        next_max = np.max(q_table[next_state])\n", "        \n", "        # код для Q-обновления\n", "        # new_value = \n", "        ####### Здесь ваш код ##########\n", "        raise NotImplementedError\n", "        ################################\n", "        \n", "        q_table[state, action] = new_value\n", "\n", "        state = next_state\n", "        episode += 1\n", "        episode_reward += reward\n", "    rewards_batch.append(episode_reward)\n", "     \n", "    if i % 100 == 0:\n", "        show_progress(rewards_batch, log)\n", "        rewards_batch = []\n", "        print(f\"Episode: {i}, Reward: {episode_reward}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "QCvfdLORQOf0"}, "source": ["Если все сделано правильно, то график должен выйти на плато около 0. А значение вознаграждение будет в диапазоне [-5, 10], за счет случайного выбора начальной позиции такси и пассажира. Попробуйте изменить гиперпараметры и сравните результаты."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "f7sfWBeBq8Wx"}, "source": ["## 4. Аппроксимация Q-функции\n", "\n", "В данном пункте мы будем использовать библиотеку tensorflow для обучения нейронной сети, хотя можно использовать и любую другую библиотеку."]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "5BFkc4eN16Lh"}, "outputs": [], "source": ["import sys, os\n", "if 'google.colab' in sys.modules:\n", "    %tensorflow_version 1.x\n", "import pandas as pd"]}, {"cell_type": "code", "execution_count": 57, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 286}, "colab_type": "code", "id": "YRnOxiAZrOFN", "outputId": "4eec6d45-f7a1-448b-fcf8-6fd6382149da"}, "outputs": [], "source": ["env = gym.make(\"CartPole-v0\").env\n", "env.reset()\n", "n_actions = env.action_space.n\n", "state_dim = env.observation_space.shape"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "cYbIV7w42Fp1"}, "source": ["Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n", "<img src=\"https://github.com/hse-ds/iad-applied-ds/raw/master/2020/seminars/seminar15/qapp.png\">\n", "Для начала попробуйте использовать только полносвязные слои (L.Dense) и простые активационные функции. Сигмоиды и другие функции не будут работать с ненормализованными входными данными."]}, {"cell_type": "code", "execution_count": 58, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 70}, "colab_type": "code", "id": "4kjlhlAP4rcf", "outputId": "c19910db-1ad2-4667-d4ae-bb257f010900"}, "outputs": [], "source": ["import tensorflow as tf\n", "import keras\n", "import keras.layers as L\n", "tf.reset_default_graph()\n", "sess = tf.InteractiveSession()\n", "keras.backend.set_session(sess)"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "yjFU7TXz4sD7"}, "outputs": [], "source": ["network = keras.models.Sequential()\n", "network.add(L.InputLayer(state_dim))\n", "# определяем граф вычислений!\n", "####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "rcZhESiN4zuE"}, "outputs": [], "source": ["import random\n", "def get_action(state, epsilon=0):\n", "    \"\"\"\n", "    сэмплируем (eps greedy) действие  \n", "    \"\"\"\n", "    q_values = network.predict(state[None])[0]\n", "    \n", "    # нужно выбрать действия eps-жадно, как мы делали в табличном Q-обучении\n", "    # action = \n", "    ####### Здесь ваш код ##########\n", "    raise NotImplementedError\n", "    ################################\n", "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n", "    \n", "    return action"]}, {"cell_type": "code", "execution_count": 61, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 84}, "colab_type": "code", "id": "LpYxHgz642cZ", "outputId": "57dec09c-b955-4cd5-db13-f91ce59436d3"}, "outputs": [], "source": ["assert network.output_shape == (None, n_actions), \"Убедитесь, что стратегия переводит s -> [Q(s,a0), ..., Q(s, a_last)]\"\n", "assert network.layers[-1].activation == keras.activations.linear, \"убедитесь, что вы предсказываете q без нелинейности\"\n", "\n", "# test epsilon-greedy exploration\n", "s = env.reset()\n", "assert np.shape(get_action(s)) == (), \"убедитесь, что возвращается только одно действие (типа integer)\"\n", "for eps in [0., 0.1, 0.5, 1.0]:\n", "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n", "    best_action = state_frequencies.argmax()\n", "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n", "    for other_action in range(n_actions):\n", "        if other_action != best_action:\n", "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n", "    print('e=%.1f tests passed'%eps)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JcAUZG5S5LmN"}, "source": ["Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n", "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2,$$\n", "где\n", "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние \n", "* $\\gamma$ дисконтирующий множетель.\n", "\n", "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Эта та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. Для этого используется функция tf.stop_gradient."]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "hpZjPiKP51_5"}, "outputs": [], "source": ["# Создаем placeholders для <s, a, r, s'>, \n", "# не забываем про индикатор окончания эпизода (is_done = True)\n", "states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim)\n", "actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n", "rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n", "next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim)\n", "is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "0MqsHE1w59un"}, "outputs": [], "source": ["# получаем q для всех действий, в текущем состоянии\n", "predicted_qvalues = network(states_ph)\n", "\n", "# получаем q-values для выбранного действия\n", "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "Gw_S5wXc6FUs"}, "outputs": [], "source": ["gamma = 0.99\n", "\n", "# применяем сеть для получения q-value для next_states_ph\n", "# predicted_next_qvalues =\n", "####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################\n", "\n", "# вычисляем V*(next_states) \n", "# по предсказанным следующим q-values\n", "# next_state_values =\n", "####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################\n", "\n", "# Вычисляем target q-values для функции потерь \n", "# target_qvalues_for_actions = \n", "####### Здесь ваш код ##########\n", "raise NotImplementedError\n", "################################\n", "\n", "# для последнего значения в эпизоде используем упрощенную формулу Q(s,a) = r(s,a) \n", "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "xO26mQWF61Rd"}, "outputs": [], "source": ["# mean squared error loss, который будем минимизировать\n", "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n", "loss = tf.reduce_mean(loss)\n", "\n", "# применяем AdamOptimizer\n", "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "VTImrMM77CTg"}, "outputs": [], "source": ["assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"убедитесь, что обновление выполняется только для выбранного действия\"\n", "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"убедитесь, что вы не распространяете градиент Q_(s',a')\"\n", "assert predicted_next_qvalues.shape.ndims == 2, \"убедитесь, что вы предсказываете q для всех действий,следующего состояния\"\n", "assert next_state_values.shape.ndims == 1, \"убедитесь, что вы вычислили V(s') как максимум только по оси действий, а не по всем осям\"\n", "assert target_qvalues_for_actions.shape.ndims == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\""]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "fCxjK8hM7NQs"}, "outputs": [], "source": ["sess.run(tf.global_variables_initializer())"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "EqcQCsXb7lAH"}, "outputs": [], "source": ["def generate_session(env, t_max=1000, epsilon=0, train=False):\n", "    total_reward = 0\n", "    s = env.reset()\n", "    \n", "    for t in range(t_max):\n", "        a = get_action(s, epsilon=epsilon)       \n", "        next_s, r, done, _ = env.step(a)\n", "        \n", "        if train:\n", "            sess.run(train_step,{\n", "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n", "                next_states_ph: [next_s], is_done_ph: [done]\n", "            })\n", "\n", "        total_reward += r\n", "        s = next_s\n", "        if done:\n", "            break\n", "            \n", "    return total_reward"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"colab": {}, "colab_type": "code", "id": "E1BN4lbq7pe4"}, "outputs": [], "source": ["epsilon = 0.5"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 391}, "colab_type": "code", "id": "5XzMJKbP7s7g", "outputId": "7ca3671f-f0db-4af9-f310-4e8ed8a865c2"}, "outputs": [], "source": ["for i in range(1, 1000):\n", "    session_rewards = [generate_session(env, epsilon=epsilon, train=True) for _ in range(100)]\n", "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n", "    if i % 10 == 0:\n", "          clear_output(wait=True)\n", "    epsilon *= 0.99\n", "    assert epsilon >= 1e-4, \"убедитесь, что epsilon не становится < 0\"\n", "    \n", "    if np.mean(session_rewards) > 300:\n", "        print(\"Принято!\")\n", "        break"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "AM3vch0F9LuN"}, "source": ["Интерпретация результатов\n", "* mean reward - среднее вознаграждение за эпизод. В случае корректной реализации, этот показатель будет низким первые 5 эпох и только затем будет возрастать и сойдется на 20-30 эпохе в зависимости от архитектуры сети. \n", "* Если сеть не достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте $\\epsilon$. \n", "* epsilon -- обеспечивает стремление агента исследовать среду. Можно искусственно изменять малые значения $\\epsilon$ при низких результатах на 0.1 - 0.5. "]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "7FVcOWucSMrU"}, "source": ["Полезные ссылки:\n", "* https://github.com/yandexdataschool/Practical_RL\n", "* https://www.youtube.com/playlist?list=PLbWDNovNB5mqFBgq7i3MY6Ui4zudcvNFJ"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["\n", "\n", ""]}], "metadata": {"colab": {"collapsed_sections": [], "name": "sem15.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.5"}}, "nbformat": 4, "nbformat_minor": 1}