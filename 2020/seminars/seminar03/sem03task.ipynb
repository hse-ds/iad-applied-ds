{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wdnw3tE9Deb1"
   },
   "source": [
    "# Майнор ВШЭ \n",
    "\n",
    "## Прикладные задачи анализа данных 2020\n",
    "\n",
    "## Семинар 3: Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EG1IgkJFLSnW"
   },
   "source": [
    "## Matrix factorization algorithm\n",
    "\n",
    "NCF - это нейронная модель матричной факторизации, которая объединяет Generalized Matrix Factorization (GMF) и Multi-Layer Perceptron (MLP), объединяя в себе сильные стороны линейности MF и нелинейности MLP для моделирования скрытых структур user-item.\n",
    "\n",
    "Схема архитектуры NCF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGfys8qiLTs7"
   },
   "source": [
    "<img src=\"https://recodatasets.blob.core.windows.net/images/NCF.svg?sanitize=true\">\n",
    "\n",
    "На схеме видно, как используются скрытые вектора пользователей и айтемов и как затем объединяются выходы из слоя GMF (слева) и слоя MLP (справа).\n",
    "\n",
    "### 1.1 Модель GMF\n",
    "\n",
    "В ALS, матрицу оценок можно записать как:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = q _ { i } ^ { T } p _ { u }$$\n",
    "\n",
    "GMF представляет собой слой NCF как стандартный выходной слой MF. Поэтому MF может быть легко обобщена и расширена. Например, если мы позволим веса ребер выходно слоя обучаться без общего ограничения - это даст вариант MF, который позволяет варьировать важность скрытых измерений. А если мы будем использовать нелинейную функцию активации, это даст обобщение MF до нелинейной формы, которая может быть более выразительной чем линейная MF модель. GMF может быть записана как:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = a _ { o u t } \\left( h ^ { T } \\left( q _ { i } \\odot p _ { u } \\right) \\right)$$\n",
    "\n",
    "где $\\odot$ - поэлементное произведение векторов, ${a}_{out}$ и ${h}$ обозначают функцию активации и веса ребер выходного слоя соответственно. MF может рассматриваться как частный случай GMF. Интуитивно, если мы используем тождественную функцию для ${a}_{out}$ и выбираем единичный вектор в качестве ${h}$, то мы в точности повторяем модель MF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHuih2qGLppl"
   },
   "source": [
    "\n",
    "### 1.2 Модель MLP\n",
    "\n",
    "NCF использует два способа при моделировании рейтингов:\n",
    "\n",
    "1) поэлементное произведение векторов,\n",
    "2) конкатенация векторов.\n",
    "\n",
    "Сразу после контатенации скрытых признаков пользователей и айтемов применяется стандартная модель MLP. Это дает возможность наделить модель большим уровнем гибкости и нелинейности для изучения взаимодействий между $p_{u}$ и $q_{i}$. \n",
    "\n",
    "Запишем модель MLP модель более строго:\n",
    "\n",
    "Для входного слоя, используется конкатенация векторов пользователей и айтемов:\n",
    "\n",
    "$$z _ { 1 } = \\phi _ { 1 } \\left( p _ { u } , q _ { i } \\right) = \\left[ \\begin{array} { c } { p _ { u } } \\\\ { q _ { i } } \\end{array} \\right]$$\n",
    "\n",
    "Для скрытых и выходного слоев MLP запись имеет вид:\n",
    "\n",
    "$$\n",
    "\\phi _ { l } \\left( z _ { l } \\right) = a _ { o u t } \\left( W _ { l } ^ { T } z _ { l } + b _ { l } \\right) , ( l = 2,3 , \\ldots , L - 1 )\n",
    "$$\n",
    "\n",
    "и:\n",
    "\n",
    "$$\n",
    "\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\phi \\left( z _ { L - 1 } \\right) \\right)\n",
    "$$\n",
    "\n",
    "где ${ W }_{ l }$, ${ b }_{ l }$, и ${ a }_{ out }$ обозначают матрицу весов, вектор свободных членов и функцию активации для $l$-ого слоя, соответственно. В качестве функций активации MLP слоев, мы вольны выбирать любую: сигмоиду, гиперболический тангенс, ReLU и другие. В качестве функции активации на выходном слое используется сигмоида $\\sigma(x)=\\frac{1}{1+e^{-x}}$, чтобы ограничить оценки диапазоном (0,1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SmWKJn1Lt7y"
   },
   "source": [
    "### 1.3 Смешивание GMF и MLP\n",
    "\n",
    "Чтобы обеспечить большую гибкость нашей смешанной модели мы позволяем GMF и MLP обучать независимые эмбединги и затем комбинируем две модели объединяя их последние скрытие слои. Мы взяли $\\phi^{GMF}$ из GMF:\n",
    "\n",
    "$$\\phi _ { u , i } ^ { G M F } = p _ { u } ^ { G M F } \\odot q _ { i } ^ { G M F }$$\n",
    "\n",
    "и получили $\\phi^{MLP}$ из MLP:\n",
    "\n",
    "$$\\phi _ { u , i } ^ { M L P } = a _ { o u t } \\left( W _ { L } ^ { T } \\left( a _ { o u t } \\left( \\ldots a _ { o u t } \\left( W _ { 2 } ^ { T } \\left[ \\begin{array} { c } { p _ { u } ^ { M L P } } \\\\ { q _ { i } ^ { M L P } } \\end{array} \\right] + b _ { 2 } \\right) \\ldots \\right) \\right) + b _ { L }\\right.$$\n",
    "\n",
    "Наконец, мы смешали выходы из GMF и MLP:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\left[ \\begin{array} { l } { \\phi ^ { G M F } } \\\\ { \\phi ^ { M L P } } \\end{array} \\right] \\right)$$\n",
    "\n",
    "Модель сочетает линейность MF и нелинейность DNN при моделировании скрытых user–item структур."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SDJwiFk8LwXr"
   },
   "source": [
    "### 1.4 Целевая функция\n",
    "\n",
    "Мы можем записать функцию правдоподобия как:\n",
    "\n",
    "$$P \\left( \\mathcal { R } , \\mathcal { R } ^ { - } | \\mathbf { P } , \\mathbf { Q } , \\Theta \\right) = \\prod _ { ( u , i ) \\in \\mathcal { R } } \\hat { r } _ { u , i } \\prod _ { ( u , j ) \\in \\mathcal { R } ^{ - } } \\left( 1 - \\hat { r } _ { u , j } \\right)$$\n",
    "\n",
    "Где $\\mathcal{R}$ обозначает множество наблюдаемых взаимодействий пользователя, а $\\mathcal{ R } ^ { - }$ обозначает множество негативных наблюдений. $\\mathbf{P}$ и $\\mathbf{Q}$ - это скрытая матрица признаков пользователей и айтемов соответственно, $\\Theta$ - параметры модели. Взяв со знаком минус логарифм от правдоподобия мы получим целевую функцию для минимизации NCF алгоритма. Что-то напоминает, не правда ли? https://en.wikipedia.org/wiki/Cross_entropy\n",
    "\n",
    "$$L = - \\sum _ { ( u , i ) \\in \\mathcal { R } \\cup { \\mathcal { R } } ^ { - } } r _ { u , i } \\log \\hat { r } _ { u , i } + \\left( 1 - r _ { u , i } \\right) \\log \\left( 1 - \\hat { r } _ { u , i } \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opNrJeB32qv7"
   },
   "source": [
    "## Задания для самостоятельной работы:\n",
    "\n",
    "(на каждом шаге оценивая качество и время обучения)\n",
    "1. Собрать архитектуру сети только на GMF\n",
    "1. Собрать архитектуру сети только на MLP\n",
    "1. Собрать архитектуру NeuMF (GMF+MLP)\n",
    "\n",
    "опционально:\n",
    "1. Реализовать Early stopping\n",
    "1. Реализовать сохранение и загрузку pre-train GMF и MLP моделей\n",
    "1. Модифицировать модель в сторону усложнения архитектуры сети с помощью параметров: model_layers, mlp_reg_layers\n",
    "1. Попробовать различные гиперпараметры: mf_regularization, lr\n",
    "1. Попробовать различные оптимизаторы: SGD, Adam и др.\n",
    "1. Реализовать сохранение обученной модели с лучшим качеством"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAyBExMGLWJC"
   },
   "source": [
    "## Tensorflow NCF implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YdVsP6LElUF"
   },
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "colab_type": "code",
    "id": "SyKykR2fEqDB",
    "outputId": "81ceafe6-cab5-4237-877e-4679993db1be"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.negative\n",
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.rating\n",
    "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.train.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3a-EgDuDYUw"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "_UL1pN8cfG4j",
    "outputId": "274a02c5-2fd1-4f02-b86f-2b46b805cc11"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "import scipy.sparse as sp\n",
    "import multiprocessing\n",
    "from six.moves import xrange\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32-nxuu6Db6M"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORrCQPT4uZgl"
   },
   "outputs": [],
   "source": [
    "# ml-1m dataset contains 1,000,209 anonymous ratings of approximately 3,706 movies made by 6,040 users who joined MovieLens in 2000.\n",
    "# All ratings are contained in the file \"ratings.dat\" without header row, and are in the following format:\n",
    "# UserID::MovieID::Rating::Timestamp\n",
    "#\n",
    "# - UserIDs range between 1 and 6040.\n",
    "# - MovieIDs range between 1 and 3952.\n",
    "# - Ratings are made on a 5-star scale (whole-star ratings only).\n",
    "\n",
    "FILE_NAME = 'ml-1m'\n",
    "\n",
    "USER_COLUMN = \"user_id\"\n",
    "ITEM_COLUMN = \"item_id\"  # movies\n",
    "RATING_COLUMN = \"rating\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSNns4ibJCj6"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7hN0q1KZFGw"
   },
   "source": [
    "Данные предварительно предобработаны:\n",
    "\n",
    "**train.rating:**\n",
    "- Train file.\n",
    "- Each Line is a training instance: userID itemID rating timestamp (if have)\n",
    "\n",
    "**test.rating:**\n",
    "- Test file (positive instances). \n",
    "- Each Line is a testing instance: userID itemID rating timestamp (if have)\n",
    "\n",
    "**test.negative**\n",
    "- Test file (negative instances).\n",
    "- Each line corresponds to the line of test.rating, containing 99 negative samples.  \n",
    "- Each line is in the format: (userID,itemID) negativeItemID1 negativeItemID2 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_6RifGb_rI3"
   },
   "outputs": [],
   "source": [
    "### Ничего интересного - чтение данных из файлов (можно пропустить).\n",
    "\n",
    "def load_rating_file_as_list(filename):\n",
    "    ratingList = []\n",
    "    with open(filename+'.test.rating', \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item = int(arr[0]), int(arr[1])\n",
    "            ratingList.append([user, item])\n",
    "            line = f.readline()\n",
    "    return ratingList\n",
    "\n",
    "def load_negative_file(filename):\n",
    "    negativeList = []\n",
    "    with open(filename+'.test.negative', \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            for x in arr[1: ]:\n",
    "                negatives.append(int(x))\n",
    "            negativeList.append(negatives)\n",
    "            line = f.readline()\n",
    "    return negativeList\n",
    "\n",
    "def load_rating_file_as_matrix(filename):\n",
    "    '''\n",
    "    Read .rating file and Return dok matrix.\n",
    "    '''\n",
    "    # Get number of users and items\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename+'.train.rating', \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    # Construct matrix\n",
    "    mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    with open(filename+'.train.rating', \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            if (rating > 0):\n",
    "                mat[user, item] = 1.0\n",
    "            line = f.readline()    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MZE5OBXGAdyz",
    "outputId": "2516de6f-007c-4c56-cebc-851a75ac5240"
   },
   "outputs": [],
   "source": [
    "t1 = time()\n",
    "\n",
    "# Loading data\n",
    "train, testRatings, testNegatives = load_rating_file_as_matrix(FILE_NAME), load_rating_file_as_list(FILE_NAME), load_negative_file(FILE_NAME)\n",
    "num_users, num_items = train.shape\n",
    "\n",
    "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "      %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "134bmv7QEDhR"
   },
   "source": [
    "### Metrics functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsjzYANX9cJ6"
   },
   "outputs": [],
   "source": [
    "### Расчет метрик: Hit_Ratio, NDCG для top-K рекомендаций\n",
    "\n",
    "# Global variables that are shared across processes\n",
    "_model = None\n",
    "_testRatings = None\n",
    "_testNegatives = None\n",
    "_K = None\n",
    "\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in xrange(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def eval_one_rating(idx):\n",
    "    rating = _testRatings[idx]\n",
    "    items = _testNegatives[idx]\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    items.append(gtItem)\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype = 'int32')\n",
    "    predictions = _model.predict([users, np.array(items)], \n",
    "                                 batch_size=100, verbose=0)\n",
    "    for i in xrange(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    items.pop()\n",
    "    \n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    return (hr, ndcg)\n",
    "\n",
    "def evaluate_model(model, testRatings, testNegatives, K, num_thread):\n",
    "\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _K\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _K = K\n",
    "        \n",
    "    hits, ndcgs = [],[]\n",
    "    if(num_thread > 1): # Multi-thread\n",
    "        pool = multiprocessing.Pool(processes=num_thread)\n",
    "        res = pool.map(eval_one_rating, range(len(_testRatings)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        hits = [r[0] for r in res]\n",
    "        ndcgs = [r[1] for r in res]\n",
    "        return (hits, ndcgs)\n",
    "    # Single thread\n",
    "    for idx in xrange(len(_testRatings)):\n",
    "        (hr,ndcg) = eval_one_rating(idx)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)      \n",
    "    return (hits, ndcgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFoBFLhgPhIu"
   },
   "source": [
    "<img src=\"https://recodatasets.blob.core.windows.net/images/NCF.svg?sanitize=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQC7ggbEJXSo"
   },
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2eCKo2fnfsFs"
   },
   "outputs": [],
   "source": [
    "mf_regularization = 0. #regularization factor for MF embeddings\n",
    "\n",
    "def get_model(num_users, num_items, mf_dim=10, model_layers=[32, 32, 16, 8, 8], mlp_reg_layers=[0., 0.], reg_mf=0):\n",
    "    assert len(model_layers) == len(mlp_reg_layers)\n",
    "\n",
    "    # Input variables\n",
    "\n",
    "    user_input = tf.keras.layers.Input(\n",
    "      shape=(1,), name=USER_COLUMN, dtype=tf.int32)\n",
    "    \n",
    "    item_input = tf.keras.layers.Input(\n",
    "      shape=(1,), name=ITEM_COLUMN, dtype=tf.int32)\n",
    "\n",
    "    # Embedding layer\n",
    "\n",
    "    # Initializer for embedding layers\n",
    "    embedding_initializer = \"glorot_uniform\"\n",
    "\n",
    "   # It turns out to be significantly more effecient to store the MF and MLP\n",
    "    # embedding portions in the same table, and then slice as needed.\n",
    "    embedding_user = tf.keras.layers.Embedding(\n",
    "        num_users,\n",
    "        mf_dim + model_layers[0] // 2,\n",
    "        embeddings_initializer=embedding_initializer,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(mf_regularization),\n",
    "        input_length=1,\n",
    "        name=\"embedding_user\")(\n",
    "            user_input)\n",
    "\n",
    "    embedding_item = tf.keras.layers.Embedding(\n",
    "        num_items,\n",
    "        mf_dim + model_layers[0] // 2,\n",
    "        embeddings_initializer=embedding_initializer,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(mf_regularization),\n",
    "        input_length=1,\n",
    "        name=\"embedding_item\")(\n",
    "            item_input)\n",
    "\n",
    "    # GMF part\n",
    "\n",
    "    def mf_slice_fn(x):\n",
    "       \"\"\" your code here \"\"\" \n",
    "    mf_user_latent = tf.keras.layers.Lambda( \"\"\" your code here \"\"\" )\n",
    "    mf_item_latent = tf.keras.layers.Lambda( \"\"\" your code here \"\"\" )\n",
    "    \n",
    "    # Element-wise multiply\n",
    "    mf_vector =  \"\"\" your code here \"\"\" \n",
    "\n",
    "    # MLP part\n",
    "\n",
    "    def mlp_slice_fn(x):\n",
    "       \"\"\" your code here \"\"\" \n",
    "    mlp_user_latent = tf.keras.layers.Lambda( \"\"\" your code here \"\"\" )\n",
    "    mlp_item_latent = tf.keras.layers.Lambda( \"\"\" your code here \"\"\" )\n",
    "\n",
    "    # Concatenation of two latent features\n",
    "    mlp_vector =  \"\"\" your code here \"\"\" \n",
    "\n",
    "    num_layer = len(model_layers) # Number of layers in the MLP\n",
    "    for layer in xrange(1, num_layer):\n",
    "       \"\"\" your code here \"\"\" \n",
    "\n",
    "    # Concatenate GMF and MLP parts\n",
    "    predict_vector =  \"\"\" your code here \"\"\"   \n",
    "    \n",
    "    # Final prediction layer\n",
    "    logits = tf.keras.layers.Dense(\n",
    "        1, activation=None, kernel_initializer=\"lecun_uniform\",\n",
    "        name=RATING_COLUMN)(predict_vector)\n",
    "\n",
    "    model = tf.keras.models.Model([user_input, item_input], logits) \n",
    "\n",
    "    # Print model topology.\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "YaZg7_TH7FT8",
    "outputId": "a92562c2-21e5-4b9e-db17-58b8548fe602"
   },
   "outputs": [],
   "source": [
    "model = get_model(num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blhjtveG7Rhn"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.1), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I17ntIP8OdEP"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "K9SIDjZWA8so",
    "outputId": "2c129b45-97c8-422b-a04e-1d62d7cb4abe"
   },
   "outputs": [],
   "source": [
    "# обучается ~ 2 мин.\n",
    "start_time = time()\n",
    "\n",
    "# Init performance\n",
    "topK = 10\n",
    "evaluation_threads = 1 #multiprocessing.cpu_count()\n",
    "\n",
    "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "print(\"Init: HR = %.4f, NDCG = %.4f\" % (hr, ndcg))\n",
    "\n",
    "train_time = time() - start_time\n",
    "print(\"Took %.1f seconds for training.\" % (train_time))\n",
    "\n",
    "best_hr, best_ndcg, best_iter = hr, ndcg, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVSPsbGNPyND"
   },
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in xrange(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while (u, j) in train:\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "HjN_V5CH75P2",
    "outputId": "df6dd15b-afcb-401d-f5fb-58e590954a29"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1 #20\n",
    "num_negatives = 10\n",
    "batch_size = 256\n",
    "verbose = 1\n",
    "\n",
    "# Training model\n",
    "for epoch in xrange(num_epochs):\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "    \n",
    "    # Training\n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                      np.array(labels), # labels \n",
    "                      batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
    "    t2 = time()\n",
    "    \n",
    "    # Evaluation\n",
    "    if epoch % verbose == 0:\n",
    "        (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
    "        hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
    "        print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
    "              % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
    "        if hr > best_hr:\n",
    "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
    "\n",
    "print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMLt_wYeMQ3f"
   },
   "source": [
    "## Полезные ссылки:\n",
    "\n",
    "### Основная статья:\n",
    "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu & Tat-Seng Chua, Neural Collaborative Filtering, 2017 https://arxiv.org/abs/1708.05031\n",
    "\n",
    "### Туториалы и реализации:\n",
    "##### Microsoft:\n",
    "https://github.com/microsoft/recommenders/blob/master/notebooks/02_model/ncf_deep_dive.ipynb\n",
    "\n",
    "##### Tensorflow:\n",
    "https://github.com/tensorflow/models/tree/master/official/recommendation\n",
    "\n",
    "##### Towards Data Science\n",
    "https://towardsdatascience.com/neural-collaborative-filtering-96cef1009401\n",
    "\n",
    "### Лекции:\n",
    "Е.Соколов. Рекомендательные системы. Лекция 1 https://github.com/hse-ds/iad-applied-ds/blob/master/2020/lectures/lecture01-recommender.pdf\n",
    "\n",
    "Е.Соколов. Рекомендательные системы. Лекция 2 https://github.com/hse-ds/iad-applied-ds/blob/master/2020/lectures/lecture02-recommender.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NCF_stud",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
