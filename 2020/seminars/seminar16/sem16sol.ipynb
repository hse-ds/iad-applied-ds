{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "XkG_ZcvCuQJ0"}, "source": ["# Майнор ВШЭ\n", "## Прикладные задачи анализа данных 2020\n", "## Обучение с подкреплением: CrossEntropy Method (CEM), Deep CEM."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "UM7vcYORuQJ1"}, "source": ["## 1. Crossentropy Method\n", "\n", "В этой пункте мы посмотрим на то, как решить задачи RL с помощью метода crossentropy.\n", "\n", "Рассмотрим пример с задачей Taxi [Dietterich, 2000]. "]}, {"cell_type": "code", "execution_count": 1, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 153}, "colab_type": "code", "id": "i5yU3RZuuQJ2", "outputId": "95a3ba3f-505e-4a54-bc45-94264b75f87d"}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import gym\n", "\n", "env = gym.make(\"Taxi-v3\")\n", "env.reset()\n", "env.render()"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "id": "Xyi1DDjQuQJ8", "outputId": "22e8e06b-156a-4b9b-888f-fffd17009f3b"}, "outputs": [], "source": ["n_states  = env.observation_space.n\n", "n_actions = env.action_space.n  \n", "\n", "print(f\"состояний: {n_states} действий: {n_actions}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "hR-L-rHquQKA"}, "source": ["В этот раз нашей стратегией будет вероятностной распределение. \n", "\n", "$\\pi(s,a) = P(a|s)$\n", "\n", "Для задачи такси мы можем использовать таблицу: \n", "\n", "policy[s,a] = P(выбрать действие a | в состоянии s)\n", "\n", "Создадим \"равномерную\" стратегию в виде двумерного массива с \n", "равномерным распределением по действиям и сгенерируем игровую сессию с такой стратегией"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"colab": {}, "colab_type": "code", "id": "sVBlXo-huQKB"}, "outputs": [], "source": ["def initialize_policy(n_states, n_actions):\n", "    ####### Здесь ваш код ##########\n", "    policy = np.array([[1./n_actions for _ in range(n_actions)] for _ in range(n_states)])\n", "    ################################\n", "    return policy\n", "\n", "policy = initialize_policy(n_states, n_actions)"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"colab": {}, "colab_type": "code", "id": "-xqVJHT4uQKE"}, "outputs": [], "source": ["assert type(policy) in (np.ndarray, np.matrix)\n", "assert np.allclose(policy, 1./n_actions)\n", "assert np.allclose(np.sum(policy, axis=1), 1)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "iDx_ek7vuQKH"}, "source": ["### Генерация сессий взаимодейтсвия со средой.\n", "\n", "Мы будем запоминать все состояния, действия и вознаграждения за эпизод."]}, {"cell_type": "code", "execution_count": 5, "metadata": {"colab": {}, "colab_type": "code", "id": "QlsAeF7EuQKI"}, "outputs": [], "source": ["def generate_session(env, policy, t_max=10**4):\n", "    \"\"\"\n", "    Игра идет до конца эпизода или до t_max шагов в окружении. \n", "    :param policy: [n_states,n_actions] \n", "    :returns: states - список состояний, actions - список действий, total_reward - итоговое вознаграждение\n", "    \"\"\"\n", "    states, actions = [], []\n", "    total_reward = 0.\n", "\n", "    s = env.reset()\n", "\n", "    for t in range(t_max):\n", "        # Hint: you can use np.random.choice for sampling action\n", "        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n", "        # a = \n", "        ####### Здесь ваш код ########## \n", "        a = np.random.choice(n_actions, p=policy[s])\n", "        ################################\n", "\n", "        new_s, r, done, info = env.step(a)\n", "\n", "        # Record information we just got from the environment.\n", "        states.append(s)\n", "        actions.append(a)\n", "        total_reward += r\n", "\n", "        s = new_s\n", "        if done:\n", "            break\n", "\n", "    return states, actions, total_reward"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"colab": {}, "colab_type": "code", "id": "HM2-QI3UuQKL"}, "outputs": [], "source": ["s, a, r = generate_session(env, policy)\n", "assert type(s) == type(a) == list\n", "assert len(s) == len(a)\n", "assert type(r) in [float, np.float]"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 282}, "colab_type": "code", "id": "gWqyUSLJuQKP", "outputId": "277a831b-2c42-42b1-a8f4-516e3c508c7d"}, "outputs": [], "source": ["# посмотрим на изначальное распределение вознаграждения\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "sample_rewards = [generate_session(env, policy, t_max=1000)[-1] for _ in range(200)]\n", "\n", "plt.hist(sample_rewards, bins=20)\n", "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n", "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n", "plt.legend()"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "hUDOpKxHuQKS"}, "source": ["### Реализация метода crossentropy  \n", "\n", "Наша задача - выделить лучшие действия и состояния, т.е. такие, при которых было лучшее вознаграждение:"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"colab": {}, "colab_type": "code", "id": "FGIweAi5uQKT"}, "outputs": [], "source": ["def select_elites(states_batch, actions_batch, \n", "                  rewards_batch, percentile=50):\n", "    \"\"\"\n", "    Выбирает состояния и действия с заданным перцентилем (rewards >= percentile)\n", "    :param states_batch: list of lists of states, states_batch[session_i][t]\n", "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n", "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n", "    \n", "    :returns: elite_states, elite_actions - одномерные \n", "    списки состояния и действия, выбранных сессий\n", "    \"\"\"\n", "    # нужно найти порог вознаграждения по процентилю\n", "    # reward_threshold =\n", "    ####### Здесь ваш код ##########\n", "    reward_threshold = np.percentile(rewards_batch, percentile)\n", "    ################################\n", "    \n", "    \n", "    # в соответствии с найденным порогом - отобрать \n", "    # подходящие состояния и действия\n", "    # elite_states = \n", "    # elite_actions = \n", "    ####### Здесь ваш код ##########\n", "    elite_states  = [session for session_i, session in enumerate(states_batch) if rewards_batch[session_i] >= reward_threshold]\n", "    elite_states = [state for session in elite_states for state in session]\n", "    \n", "    elite_actions = [session for session_i, session in enumerate(actions_batch) if rewards_batch[session_i] >= reward_threshold]\n", "    elite_actions = [action for session in elite_actions for action in session]\n", "    ################################\n", "    \n", "    return elite_states, elite_actions"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "id": "Q34NZBHHuQKW", "outputId": "2e067944-6ca2-43fb-d5db-71a527602997"}, "outputs": [], "source": ["states_batch = [\n", "    [1, 2, 3],     # game1\n", "    [4, 2, 0, 2],  # game2\n", "    [3, 1],        # game3\n", "]\n", "\n", "actions_batch = [\n", "    [0, 2, 4],     # game1\n", "    [3, 2, 0, 1],  # game2\n", "    [3, 3],        # game3\n", "]\n", "rewards_batch = [\n", "    3,  # game1\n", "    4,  # game2\n", "    5,  # game3\n", "]\n", "\n", "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n", "test_result_30 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n", "test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n", "test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n", "\n", "assert np.all(\n", "    test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1]) \\\n", "       and np.all(\n", "    test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]), \\\n", "    \"Для процентиля 0 необходимо выбрать все состояния \" \\\n", "    \"и действия в хронологическом порядке\"\n", "\n", "assert np.all(test_result_30[0] == [4, 2, 0, 2, 3, 1])\\\n", "   and np.all(test_result_30[1] == [3, 2, 0, 1, 3, 3]), \\\n", "    \"Для процентиля 30 необходимо выбрать \" \\\n", "    \"состояния/действия из [3:]\"\n", "assert np.all(test_result_90[0] == [3, 1]) and \\\n", "       np.all(test_result_90[1] == [3, 3]), \\\n", "    \"Для процентиля 90 необходимо выбрать состояния \" \\\n", "    \"действия одной игры\"\n", "assert np.all(test_result_100[0] == [3, 1]) and \\\n", "       np.all(test_result_100[1] == [3, 3]), \\\n", "    \"Проверьте использование знаков: >=,  >. \" \\\n", "    \"Также проверьте расчет процентиля\"\n", "print(\"Тесты пройдены!\")\n"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "ZZzLzP7PuQKb"}, "source": ["Теперь мы хотим написать обновляющуюся стратегию:"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"colab": {}, "colab_type": "code", "id": "PecfT_xEuQKc"}, "outputs": [], "source": ["def update_policy(elite_states,elite_actions):\n", "    \"\"\"\n", "    Новой стратегией будет:\n", "    policy[s_i,a_i] ~ #[вхождения  si/ai в лучшие states/actions]\n", "    \n", "    Не забудьте про нормализацию состояний.\n", "    Если какое-то состояние не было посещено, \n", "    то используйте равномерное распределение 1./n_actions\n", "    \n", "    :param elite_states:  список состояний\n", "    :param elite_actions: список действий\n", "    \"\"\"\n", "    new_policy = np.zeros([n_states,n_actions])\n", "    for state in range(n_states):\n", "        # обновляем стратегию - нормируем новые частоты \n", "        # действий и не забываем про непосещенные состояния\n", "        # new_policy[state, a] =         \n", "        ####### Здесь ваш код ##########\n", "        occur = 0\n", "        for pos, s in enumerate(elite_states):\n", "            if s == state:\n", "                new_policy[state, elite_actions[pos]] += 1\n", "                occur += 1\n", "        if occur > 0:\n", "            for a in range(n_actions):\n", "                new_policy[state, a] /= occur\n", "        else:\n", "            for a in range(n_actions):\n", "                new_policy[state, a] = 1./n_actions\n", "        ################################\n", "        \n", "    return new_policy"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "id": "DsCFXHMRuQKf", "outputId": "c9a057c5-70a2-4de1-e24b-6812e4ddd7e3"}, "outputs": [], "source": ["elite_states, elite_actions = (\n", "    [1, 2, 3, 4, 2, 0, 2, 3, 1],\n", "    [0, 2, 4, 3, 2, 0, 1, 3, 3])\n", "\n", "new_policy = update_policy(elite_states, elite_actions)\n", "\n", "assert np.isfinite(\n", "    new_policy).all(), \"Стратегия не должна содержать \" \\\n", "                       \"NaNs или +-inf. Проверьте \" \\\n", "                       \"деление на ноль. \"\n", "assert np.all(\n", "    new_policy >= 0), \"Стратегия не должна содержать \" \\\n", "                      \"отрицательных вероятностей \"\n", "assert np.allclose(new_policy.sum(axis=-1),\n", "                   1), \"Суммарная\\ вероятность действий\"\\\n", "                       \"для состояния должна равняться 1\"\n", "reference_answer = np.array([\n", "    [1., 0., 0., 0., 0.],\n", "    [0.5, 0., 0., 0.5, 0.],\n", "    [0., 0.33333333, 0.66666667, 0., 0.],\n", "    [0., 0., 0., 0.5, 0.5]])\n", "assert np.allclose(new_policy[:4, :5], reference_answer)\n", "print(\"Тесты пройдены!\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "VYRLBwtvuQKi"}, "source": ["### Цикл обучения\n", "\n", "Визуализириуем наш процесс обучения и также будем измерять распределение получаемых за сессию вознаграждений "]}, {"cell_type": "code", "execution_count": 12, "metadata": {"colab": {}, "colab_type": "code", "id": "tmBnonZCuQKj"}, "outputs": [], "source": ["from IPython.display import clear_output\n", "\n", "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n", "    \"\"\"\n", "    Удобная функция, для визуализации результатов.\n", "    \"\"\"\n", "\n", "    mean_reward = np.mean(rewards_batch)\n", "    threshold = np.percentile(rewards_batch, percentile)\n", "    log.append([mean_reward, threshold])\n", "    \n", "    plt.figure(figsize=[8, 4])\n", "    plt.subplot(1, 2, 1)\n", "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n", "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n", "    plt.legend()\n", "    plt.grid()\n", "\n", "    plt.subplot(1, 2, 2)\n", "    plt.hist(rewards_batch, range=reward_range)\n", "    plt.vlines([np.percentile(rewards_batch, percentile)],\n", "               [0], [100], label=\"percentile\", color='red')\n", "    plt.legend()\n", "    plt.grid()\n", "    clear_output(True)\n", "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"colab": {}, "colab_type": "code", "id": "GXhGLvjPuQKm"}, "outputs": [], "source": ["# инициализируем стратегию\n", "policy = initialize_policy(n_states, n_actions)"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 282}, "colab_type": "code", "id": "vYncUOBfuQKo", "outputId": "2701be64-f48f-4b1c-80e9-6e194d9ad318"}, "outputs": [], "source": ["n_sessions = 250  # количество сессий для сэмплирования\n", "percentile = 50  # перцентиль \n", "learning_rate = 0.5 # то как быстро стратегия будет обновляться \n", "\n", "log = []\n", "\n", "for i in range(100):\n", "    # генерируем n_sessions сессий\n", "    # %time sessions = []\n", "    ####### Здесь ваш код ##########\n", "    sessions = [generate_session(env, policy) for _ in range(n_sessions)]\n", "    ################################\n", "    \n", "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n", "    # отбираем лучшие действия и состояния ###\n", "    # elite_states, elite_actions = \n", "    ####### Здесь ваш код ##########\n", "    elite_states, elite_actions = select_elites(\n", "    states_batch,actions_batch,rewards_batch, percentile)\n", "    ################################\n", "    \n", "    # обновляем стратегию\n", "    # new_policy =\n", "    ####### Здесь ваш код ##########\n", "    new_policy = update_policy(elite_states, elite_actions)\n", "    ################################\n", "    \n", "    policy = learning_rate * new_policy + (1 - learning_rate) * policy\n", "\n", "    # display results on chart\n", "    show_progress(rewards_batch, log, percentile)"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "3xIsESNpuQKr"}, "source": ["### Посмотрим на результаты\n", "Задача такси быстро сходится, начиня с вознаграждения -1000 к почти оптимальному значению, а потом опять падает до -50/-100. Это вызвано случайностью в самом окружении - случайное начальное состояние пассажира и такси, в начале каждого эпизода. \n", "\n", "В случае если алгоритм CEM не сможет научиться тому, как решить задачу из какого-то стартового положения, он просто отбросит этот эпизод, т.к. не будет сессий, которые переведут этот эпизод в топ лучших. \n", "\n", "Для решения этой проблемы можно уменьшить threshold (порог лучших состояний) или изменить способ оценки стратегии, используя новую стратегию, полученную из каждого начального состояния и действия (теоретически правильный способ)."]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "_-nMGpEfuQKs"}, "source": ["## 2. Deep CEM\n", "\n", "В данной части мы рассмотрим применение CEM вместе с нейронной сетью.\n", "Будем обучать многослойную нейронную сеть для решения простой задачи с непрерывным пространством действий.\n", "\n", "<img src=\"https://camo.githubusercontent.com/8f39c7f54a7798e7f80c9ec5c0bb610696e5c5b7/68747470733a2f2f7469702e64756b652e6564752f696e646570656e64656e745f6c6561726e696e672f677265656b2f6c6573736f6e2f64696767696e675f6465657065725f66696e616c2e6a7067\">"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "sWFuA1hWuQKs"}, "source": ["Будем тестировать нашего нового агента на известной задаче перевернутого маятника с непрерывным пространством состояний.\n", "https://gym.openai.com/envs/CartPole-v0/"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "id": "-ZhZahUGuQKt", "outputId": "4a1aa2a5-cbca-4b4b-f82f-9ec65e2acd96"}, "outputs": [], "source": ["import gym\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "env = gym.make(\"CartPole-v0\")\n", "\n", "env.reset()\n", "n_actions = env.action_space.n\n", "state_dim = env.observation_space.shape[0]\n", "                                        \n", "print(f\"состояний: {state_dim} действий: {n_actions}\")"]}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "UgRpAtU-uQKw"}, "source": ["### Стратегия с нейронной сетью\n", "\n", "Попробуем заменить метод обновления вероятностей на нейронную сеть. \n", "Будем пользоваться упрощенной реализацией нейронной сети из пакета Scikit-learn.\n", "Нам потребуется: \n", "* agent.partial_fit(states, actions) - делает один проход обучения по данным. Максимизирует вероятность :actions: из :states:\n", "* agent.predict_proba(states) - предсказыает вероятность каждого из действий, в виде матрицы размера [len(states), n_actions]"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 153}, "colab_type": "code", "id": "5fOPMKcYuQKx", "outputId": "d8019638-3a51-4735-9f28-b2f673ead3d6"}, "outputs": [], "source": ["from sklearn.neural_network import MLPClassifier\n", "\n", "agent = MLPClassifier(\n", "    hidden_layer_sizes=(20, 20),\n", "    activation='tanh',\n", ")\n", "\n", "# initialize agent to the dimension of state space and number of actions\n", "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))"]}, {"cell_type": "code", "execution_count": 17, "metadata": {"colab": {}, "colab_type": "code", "id": "R9W-5nV3uQK0"}, "outputs": [], "source": ["def generate_session(env, agent, t_max=1000):\n", "    \n", "    states,actions = [],[]\n", "    total_reward = 0\n", "    \n", "    s = env.reset()\n", "    \n", "    for t in range(t_max):\n", "        # предсказываем вероятности действий по сети и \n", "        # выбираем одно действие\n", "        # probs = \n", "        # a = \n", "        ####### Здесь ваш код ##########\n", "        probs = agent.predict_proba([s])[0] \n", "        a = np.random.choice(n_actions, p=probs)\n", "        ################################\n", "        \n", "        new_s,r,done,info = env.step(a)\n", "        \n", "        states.append(s)\n", "        actions.append(a)\n", "        total_reward+=r\n", "        \n", "        s = new_s\n", "        if done: break\n", "    return states,actions,total_reward"]}, {"cell_type": "code", "execution_count": 18, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 136}, "colab_type": "code", "id": "0pntOSnguQK2", "outputId": "be57e836-f5ae-4f31-e9f1-578dad7813fb"}, "outputs": [], "source": ["dummy_states, dummy_actions, dummy_reward = generate_session(env, agent, t_max=5)\n", "print(\"состояния:\", np.stack(dummy_states))\n", "print(\"действия:\", dummy_actions)\n", "print(\"вознаграждение:\", dummy_reward)"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 299}, "colab_type": "code", "id": "HVDwmDFAuQK4", "outputId": "ebf837f8-baea-4e04-a169-905c2585ad1c"}, "outputs": [], "source": ["n_sessions = 100\n", "percentile = 70\n", "log = []\n", "\n", "for i in range(100):\n", "    ####### Здесь ваш код ##########\n", "    sessions = [generate_session(env, agent) for _ in range(n_sessions)]\n", "    ################################\n", "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n", "    \n", "    ####### Здесь ваш код ##########\n", "    elite_states, elite_actions = select_elites(states_batch,actions_batch,rewards_batch, percentile)\n", "    ################################\n", "    \n", "    # обновляем стратегию, для предсказания лучших состояний\n", "    # elite_actions(y) из elite_states(X)\n", "    ####### Здесь ваш код ##########\n", "    agent.fit(elite_states, elite_actions)\n", "    ################################\n", "    \n", "    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n", "\n", "    if np.mean(rewards_batch) > 190:\n", "        print(\"Принято!\")\n", "        break"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["\n", "\n", ""]}], "metadata": {"colab": {"collapsed_sections": [], "name": "Sem16full.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.5"}}, "nbformat": 4, "nbformat_minor": 1}